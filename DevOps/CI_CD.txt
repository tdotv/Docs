								Jenkins
							===== Администрирование =====
cd /usr/share/jenkins/			
ls /usr/share/jenkins/				Смотрим .war файл с текущей версией
sudo mv jenkins.war jenkins2.440.1.war
sudo wget ***новая версия jenkins		 Скачиваем новую версию
Чтобы поставить старую версию, нужно просто переименовать обратно в jenkins.war

							===== Plugins =====
/var/lib/jenkins/plugins
Credentials
Git?
Publish Over SSH

Для создания Node:
SSH Build Agents 
SSH Agent

							===== Jobs =====
/var/lib/jenkins/workspaces
В настройках job обязательно поставить настройку discard old builds (5) MARK

							===== Slave | Node =====
Ubuntu 22.04 Сразу на slave машине нужно установить:

sudo apt-get update
sudo apt install apt-transport-https
sudo apt-get install openssh-server
sudo apt install ssh
sudo apt-get install openjdk-11-jre
ls /usr/lib/jvm
sudo apt install default-jre

Далее делаем:
mkdir ~/.ssh
ssh-keygen -t rsa -C "The access key for Jenkins slaves"
ОБЯЗАТЕЛЬНО!!! Назначить Passphrase на этом шаге (желательно сделать разные Passphrase для разных нод)!
cat id_rsa.pub > ~/.ssh/authorized_keys
cd ~/.ssh
cat id_rsa и копируем ключ.

Дополнительно, после ввода Jenkins username, сразу внизу добавил галку Treat username as secret. Также не забываем про ввод Passphrase.

Обязательно указываем несколько labels, например для первой ноды: ubuntu ubuntu-ansible ubuntu18
Use this node as much as possible
Launch via SSH	--->	Host: ***   ---> Credentials: SSH Username with private key	---> username: ubuntu	---> private key: RSA PRIVATE KEY
---> ID && Description: ssh-key	--->	Host Key Verification: Manually trusted key

Restrict where the project can be run	MARK

							===== Jenkins Cli =====
							===== Local =====
Его можно скачать из "Manage Jenkins - Jenkins CLI"

java -jar jenkins-cli.jar -auth username:password -s http://localhost:8080 who-am-i

Нужно создать юзера (необязательно), чтобы внести его данные в эту строку. Поэтому в Manage Jenkins ---> Users
username: user_cli
Password: password123
Full Name: My Service User
Email (неважно): service@something.net

Желательно использовать токен вместо пароля. 
Логинимся как user_cli и создаем токен: token-cli ---> копируем, потому что потом он исчезнет и никак не посмотришь

export JENKINS_USER_ID=myserviceuser
export JENKINS_API_TOKEN=...
env | grep JENKINS
java -jar jenkins-cli.jar -s http://localhost:8080 who-am-i

							===== Remote =====
Опять экспортим credentials
java -jar jenkins-cli.jar -s http://xxx.xxx.xxx.xxx:8080 who-am-i

							===== Commands =====
java -jar jenkins-cli.jar -s http://localhost:8080 get-job FirstJob > myjob.xml
java -jar jenkins-cli.jar -s http://localhost:8080 create-job FirstJobFromCLI < myjob.xml

							===== GitHub =====
Офк нужен плагин Git
private ssh key нужно закинуть на Jenkins
public ssh key нужно закинуть на GitHub
При добавлении ключа на Jenkins мы должны указать Username нашего GitHub и пароль (если нужно) в Passphrase

Build Triggers:
1. Trigger build remotely - выдается URL, по которому если перейти, запустится job (нормальная тема для curl)
Authentication Token: abcd<3_123
JenkinsURL/job/___/build?token=abcd<3_123
Когда мы сделаем вызов при помощи curl по этому адресу, нам потребуется аутентификация
После http:// вставляем имя и токен пользователя Jenkins http://tdotv:user-token@JenkinsURL/job/___/build?token=abcd<3_123

2. Build after other projects are build - стартуется после завершения других билдов
Например наш первый job при помощи Terraform поднимает сервер, и если сервер успешно поднялся, то мы Trigger only if build is state
запускаем второй job с деплоем. Там указывается проект, который будет отслеживать Jenkins

3. Build periodically - В какой день, в какой час запускать job

4. Poll SCM - То же самое, что и 3, но оно проверяет репозиторий всегда. Если есть изменение, то стартует job. 
		Указываем расписание проверки репозитория

5. Triggers from Plugins
К примеру благодаря плагину GitHub
Ставим галку GitHub Project	--->	SSH ссылка		| Отличие от прошлого раза добавления гита, тут выполняется триггера, 
								| а там клонирование
Ставим галку GitHub hook trigger for GITScm pooling 

В репозитории на GitHub ---> Settings ---> Webhooks ---> Add ---> Ссылка на Jenkins 
(обязательно, чтобы был открыт порт 8080 для интернета), например http://localhost:8080/github-webhook/ ---> application/json

Открыть порт 53: iptables -I INPUT -p tcp -m tcp --dport 53 -j ACCEPT	| Утилита iptables
Какие порты прослушиваются: netstat -an | grep LISTEN

							===== Build с Параметрами =====

Build с Параметрами:						| К примеру мы просматриваем директорию /etc, и нам надо теперь просматривать /var
ls -la /var							| Заходить вручную менять постоянно - плохо, job вообще трогать минимально
Ставим галочку на this project is parameterized (Choice или String (Password Parameter херня полная)). 
Все данные указываем большими буквами, потому что они могут использоваться потом как Environmental Variable
Name: FOLDERNAME
Default Value: /home
Description: Please enter Linux folder name to list

Меняем скрипт: ls -la $FOLDERNAME

							===== Deploy в AWS Elastic Beanstalk =====
Services ---> Elastic Beanstalk ---> Create Web App --->  Application Name: Jenkins App; Platform: к примеру PHP; Application Code: Sample; ---> Configure More Options ---> High Availability 
!---> Capacity Edit ---> Instances: min 2 - max 4 ---> Metric: CPU Utilization ---> Unit: Percent ---> Upper threshold: 80 && Lower threshold: 20 ---> Save
!---> Rolling updates and deployment ---> Deployment Policy: Rolling ---> Rolling update type: Rolling based on Health

Создаем User в AWS для Jenkins
IAM ---> Users ---> User Name: jenkins ---> Attach existing polices ---> Next ---> Create User (Копируем Acess Key ID && Secret Key)

Создаем репозиторий ---> Закидываем туда Sample Application Code из AWS

Jenkins
Скачиваем плагин (AWS Elastic Beanstalk) AWSEB Deployment
Description: This job will deploy our PHP code from GitHub to AWS Elastic Beanstalk
Discard old builds: max 5
GitHub Project: Ссылка на репозиторий (URL)
Source Code Management: Git (SSH) + Credentials
GitHub hook trigger ---> Добавить Webhook

Jenkins Build:
AWS Elastic Beans ---> Credentials: AWS Credentials; ID && Description: jenkins-aws-beanstalk; И ключи, который давались раньше
		  ---> AWS Region: Регион веб-сервера, можно посмотреть в URL, например region=us-east-2 (Ohio)
		  ---> Number of Attempts: 30 
Application Name && Environment Name
Packaging ---> Root Object: .	(все файлы, больше ничего указывать не надо в этом разделе)
Version and Deployment ---> Version Label Format: Jenkins-Build-${BUILD-ID}
Save

							===== Pipeline =====
agent { docker { image 'pyhon:latest' }}

environment {
  PROJECT_NAME = "MyProject"
  OWNER_NAME = "tdotv"
}

stages {
  stage("1-Stage") {
    echo "Project Name is ${PROJECT_NAME}"
  }
}

							===== Pipeline =====
stage("docker login") {
  steps {
    echo "docker login"
    withCredentials([usernamePassword(credentialsId: 'dockerhub_semaev', usernameVariable: 'USERNAME', passwordValue: 'PASSWORD')])
    sh '''
    docker login -u $USERNAME -p $PASSWORD
    '''
  }
}
------------------------------------------------------------------------------------------------------------------------------------

Предположим у нас есть проект с docker-compose, в котором есть postgres и project. У project есть зависимость от postgres.

#docker-compose.yml
services:
  yt_postgres:
    image: postgres:15
    container_name: yt_postgres
    env_file:
      - $env
  project:
    image: yt_django:test
    build: 
      context: ../../.				# Выходу в корень докера, а потом в корень проекта. В корне проекта видим Dockerfile
    container_name: yt_django
    env_file:
      - $env
    depends_on:
      - yt_postgres
    command: >
      bash -c "sleep 10 && python manage.py test"	# Ждем 10 секунд пока Бд инициализируется и запускаем приложение
      
!!! Все настройки для Postgres к примеру описываем в файле env, который, очевидно, будет в gitignor'е. И в гитлаб раннере его тоже не будет
Поэтому стоит добавить новую переменную в типе файла и вставляем всю инфу в содержимое.

#.gitlub-ci.yml
stages:
  - test
  - delievery
  - deploy
  
test-job:
  stage: test
  script:
    - docker compose -f docker/test/docker-compose.yml down				# Убиваем возможный докер контейнер
    - docker compose -f docker/test/docker-compose.yml build project			# Принудительно билдим наш образ 
    - docker compose -f docker/test/docker-compose.yml up --abort-on-container-exit	# Когда manage.py test выполнится, то наш youtube завершит
    											  работу, а заодно и БД благодаря флагу
    											  
delievery-job:
  stage: delievery
  only:
    - main						# Будет срабатывать, только после merge в главную ветку. И пушит туда новый образ
  script:
    - docker login -p $DOCKER_REGISTRY_KEY -u $DOCKER_REGISTRY_USER registry.soaqa.ru
    - docker tag yt_django:test $CI_REGISTRY_IMAGE
    - docker push $CI_REGISTRY_IMAGE
    
Качаем новую версию образа: docker-compose pull project --> docker compose up		# Ручное CD

deploy-job:
  stage: deploy
  when: manual
  only:
    - main
  script:
    - ssh server deploy.sh								# Автоматический CD
    
											# CD используя Watchtower. В docker-compose.yml
watchtower:
  image: containrrr/watchrower
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock
    - /root/.docker/config.json:/config.json
  command: --interval 30 yt_django
  
  
Пример с курсовой GitLab:
stages: 
  - build 
  - test 
  - deploy 
 
image: docker:19.03.12 
services: 
  - docker:dind 
 
variables: 
  STACK_NAME: isnta_api 
  VERSION: ${CI_COMMIT_TAG} 
  IMAGE_NAME: ${DOCKER_REGISTRY}/${CI_PROJECT_PATH}:${CI_COMMIT_TAG} 
 
before_script: 
  - echo $STACK_NAME 
  - echo $CI_REGISTRY 
  - echo $IMAGE_NAME 
 
build_main: 
  stage: build 
  script: 
    - docker login -u "frameepower" -p "Coldmar1234" 
    - echo "Building the project..." 
    - | 
      if docker ps -a --format '{{.Names}}' | grep -q "^mycontainer_main$"; then 
        echo "Удаляем контейнер mycontainer_main" 
        docker rm -f mycontainer_main 
      else 
        echo "Контейнер mycontainer_main не найден" 
      fi 
    - docker build -t hello_world_flask:master -f python/Dockerfile . 
    - docker tag hello_world_flask:master frameepower/abibok:master 
    - docker push frameepower/abibok:master 
  rules: 
    - if: '$CI_COMMIT_BRANCH == "main"' 
 
build_feature: 
  stage: build 
  script: 
    - docker login -u "frameepower" -p "Coldmar1234" 
    - | 
      if docker ps -a --format '{{.Names}}' | grep -q "^mycontainer_feature$"; then 
        echo "Удаляем контейнер mycontainer_feature" 
        docker rm -f mycontainer_feature 
      else 
        echo "Контейнер mycontainer_feature не найден" 
      fi 
    - echo "Building the project..." 
    - docker build -t hello_world_flask:feature -f python/Dockerfile . 
    - docker tag hello_world_flask:feature frameepower/abibok:feature 
    - docker push frameepower/abibok:feature 
  rules: 
    - if: '$CI_COMMIT_BRANCH == "feature"'     
 
deploy_main: 
  stage: deploy 
  script: 
    - echo "Deploying the project..." 
    - docker run -d -p 8081:80 --name mycontainer_main hello_world_flask:master 
  rules: 
    - if: '$CI_COMMIT_BRANCH == "main"'     
 
deploy_feature: 
  stage: deploy 
  script: 
    - echo "Deploying the project..." 
    - docker run -d -p 8082:80 --name mycontainer_feature hello_world_flask:feature 
  rules: 
    - if: '$CI_COMMIT_BRANCH == "feature"'

------------------------------------------------------------------------------------------------------------------------------------
						Ansible AWX - система управления конфигурациями
===== Установка =====
pip3 install ansible
git clone https://github.com/ansible/awx
nano inventory ---> раскомментировать project_data_dir=/var/lib/awx/projects
ansible-playbook install.yml -i inventory
ip a ---> go to ip address

===== Подключение проекта. Настройка и запуск. =====
		===== Manual =====
Создаем организацию - она будет группировать проекты HOME
Создаем проект - SCM Type обычно выбирают либо Manual, либо Git. Name: Create File
Manual - те playbooks, которые расположены на самом сервере AWX. /var/lib/awx/projects

Inventories - то место, где указываем хосты 
Name: home-inventory
---> Hosts ---> Add ---> HostName: client1; Organization: HOME
variables
---
ansible_host: ip-адрес клиента

Credentials - чтобы работать с хостом, нужно указать credentials
Name: client1	Organization: HOME	
CredentialType (тип данных): Machine	 Username: root		Password: *********

Можно нажать на галочку рядом с хостом, чтобы запустить команду. Например: ping; Machine Credential: client1	---> Launch
Таким образом подключается проект, теперь нужно подключить playbook

Templates ---> Add ---> 
Name: CreateFile; JobType: Run; Inventory: home-inventory; Project: Create File; Playbook: create-file.yml;
Credentials: client1; ---> Save

		===== Git =====
Credentials ---> Name: GitLab; Organization: Home; Type: Source Control: Username: tdotv, Password: *****
Projects ---> Name: test-survey; Organization: Home; Type: Git; SCM URL: https url link (только dns поменять на ip-адрес); Credential: GitLab
+ поставить галочку на Update Revision On Launch
Inventory ---> Name: Dynamic; Organization: Home; +Hosts ---> Name: dynamicHost; Variables: ansible_host:	Сохраняем
Template ---> Name: test-survey; Job: Run; Inventory: Dynamic; Project: test-survey; Playbook: update-install-user.yml; Credentials: client1;

Add Survey ---> Prompt: Install Packages; Answer Variable Name: packages (добавляем переменную с ответом, например из файла); Answer Type: Multiple
Multiple Choice: 
tree
vim
htop
nmon

		Prompt: Hostname; Answer Variable Name: hostname; Answer Type: Text
Default Asnwer: testhost;
		Prompt: New User; Answer: new_user; Type: Text
Default: user1;
		Prompt: User Password; Answer: user_pw; Type: Password
Default: password;
		Prompt: Host IP; Answer: ansible_host; Type: Text
Default: 192.168.178.xxx;
											---> Save
